---
title: "Testing nv"
author: "Collin"
date: "2024-04-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(pROC)
library(dplyr)

#algorithm specific libraries
library(MASS)
library(randomForest)
library(gbm)
library(e1071)
```



#Data

Import, organize and clean data.

I will go through and Cite / Clean up where this data is coming from. It should be cleaned up and good to use so Let me know if you have any questions about what variables mean before I get this updated.

Important Things to know:

.HOME means the stat applies to the home team and .Away means to the away team

.Off means that the data comes from the Offense data

.Pitch means that the data comes from the Pitching data

.Field means that it comes from the Fielding data

if there is no extra after .Home then it comes from the batting data

.opp means that it is the pitching data, basically calulatinfg the same stats as above just for what teams allow

.Diff means that it is the subtraction of .Home and .Away These will be what we want to use for models


```{r}

Pitching = read.csv("https://raw.githubusercontent.com/cdejong1/mlb_model/main/data/PitchingStats.csv")
Offense = read.csv("https://raw.githubusercontent.com/cdejong1/mlb_model/main/data/OffensiveStats.csv")
Fielding = read.csv("https://raw.githubusercontent.com/cdejong1/mlb_model/main/data/FieldingStats.csv")
Pitching = read.csv("https://raw.githubusercontent.com/cdejong1/mlb_model/main/data/PitchingStats.csv")
Batting_Data = read_csv("https://raw.githubusercontent.com/cdejong1/mlb_model/main/data/mlb_team_batting_stats_2023.csv")
Pitching_Data = read_csv("https://raw.githubusercontent.com/cdejong1/mlb_model/main/data/mlb_team_pitching_stats_2023.csv")

Batting_Data = Batting_Data %>% mutate(team = ifelse(grepl("D-backs", team, ignore.case = TRUE), "Diamondbacks", team))
Pitching_Data = Pitching_Data %>% mutate(team = ifelse(grepl("D-backs", team, ignore.case = TRUE), "Diamondbacks", team))

Schedule = read_csv("https://raw.githubusercontent.com/cdejong1/mlb_model/main/data/Schedule%20Data.csv")
Schedule[2430,7] = "1 - 0"
Schedule = Schedule %>%
separate(Result, into = c("Home Score", "Away Score"), sep = " - ") %>%
mutate("Home Score" = as.integer(`Home Score`),
"Away Score" = as.integer(`Away Score`),
"ScoreDiff" = as.integer(`Home Score` - `Away Score`),
"TotalPoints" = as.integer(`Home Score` + `Away Score`),
"Winner" = if_else(ScoreDiff > 0, "Home", "Away"),
"Winner" = as.factor(Winner))

two_word_cities <- c("Los Angeles", "New York", "San Francisco", "San Diego", "St. Louis", "Kansas City", "Tampa Bay")

# Custom function to split city and team based on known two-word cities
split_team <- function(team_name) {
for (city in two_word_cities) {
if (startsWith(team_name, city)) {
team <- sub(paste("^", city, " ", sep=""), "", team_name)
return(c(city, team))
}
}
# Default split for one-word city names
parts <- strsplit(team_name, " ", fixed = TRUE)[[1]]
city <- parts[1]
team <- paste(parts[-1], collapse = " ")
return(c(city, team))
}

Schedule <- Schedule %>%
rowwise() %>%
mutate("Home Key" = split_team(`Home Team`)[2],
"Away Key"= split_team(`Away Team`)[2]) %>%
ungroup()


Combined_MLB = Schedule %>% left_join(Offense, by = c("Home Team" = "Tm"), suffix = c(".Home", "")) %>%
rename_with(.fn = ~paste0(., ".Home.Off"), .cols = setdiff(names(Offense), "Tm"))%>%
left_join(Pitching, by = c("Home Team" = "Tm"), suffix = c(".Home", "")) %>%
rename_with(.fn = ~paste0(., ".Home.Pitch"), .cols = setdiff(names(Pitching), "Tm"))%>%
left_join(Fielding, by = c("Home Team" = "Tm"), suffix = c(".Home", "")) %>%
rename_with(.fn = ~paste0(., ".Home.Field"), .cols = setdiff(names(Fielding), "Tm"))%>%
left_join(Offense, by = c("Away Team" = "Tm"), suffix = c(".Away", "")) %>%
rename_with(.fn = ~paste0(., ".Away.Off"), .cols = setdiff(names(Offense), "Tm"))%>%
left_join(Pitching, by = c("Away Team" = "Tm"), suffix = c(".Away", "")) %>%
rename_with(.fn = ~paste0(., ".Away.Pitch"), .cols = setdiff(names(Pitching), "Tm"))%>%
left_join(Fielding, by = c("Away Team" = "Tm"), suffix = c(".Away", "")) %>%
rename_with(.fn = ~paste0(., ".Away.Field"), .cols = setdiff(names(Fielding), "Tm"))%>%
left_join(Batting_Data, by = c("Home Key" = "team"), suffix = c(".Home", "")) %>%
rename_with(.fn = ~paste0(., ".Home"), .cols = setdiff(names(Batting_Data), "team")) %>%
left_join(Pitching_Data, by = c("Home Key" = "team"), suffix = c(".Home", "")) %>%
rename_with(.fn = ~paste0(., ".Home.opp"), .cols = setdiff(names(Pitching_Data), "team")) %>%
left_join(Batting_Data, by = c("Away Key" = "team"), suffix = c("", ".Away")) %>%
rename_with(.fn = ~paste0(., ".Away"), .cols = setdiff(names(Batting_Data), "team")) %>%
left_join(Pitching_Data, by = c("Away Key" = "team"), suffix = c("", ".Away")) %>%
rename_with(.fn = ~paste0(., ".Away.opp"), .cols = setdiff(names(Pitching_Data), "team"))
Combined_MLB = Combined_MLB%>%
dplyr::select(-c(`Home Key`, `Away Key`, team_id.Home, team_id.Away, team_id.Home.opp, team_id.Away.opp))

base_names_Off <- c("X.Bat", "BatAge", "R.G", "G", "PA", "AB", "R", "H", "X2B", "X3B", "HR", "RBI", "SB", "CS", "BB", "SO", "BA", "OBP", "SLG", "OPS", "OPS.", "TB", "GDP", "HBP", "SH", "SF", "IBB", "LOB")

base_names_pitch <- c("X.P", "PAge", "RA.G", "W", "L", "W.L.", "ERA", "G", "GS", "GF", "CG", "tSho", "cSho", "SV", "IP", "H", "R", "ER", "HR", "BB", "IBB", "SO", "HBP", "BK", "WP", "BF", "ERA.", "FIP", "WHIP", "H9", "HR9", "BB9", "SO9", "SO.W", "LOB")

base_names_Field <- c("X.Fld", "RA.G", "DefEff", "G", "GS", "CG", "Inn","Ch", "PO", "A", "E", "DP", "Fld.", "Rtot", "Rtot.yr","Rdrs", "Rdrs.yr", "Rgood"
)
base_names <- c("attempts", "avg_hit_angle", "anglesweetspotpercent","max_hit_speed", "avg_hit_speed", "ev50", "fbld","gb", "max_distance", "avg_distance", "avg_hr_distance","ev95plus", "ev95percent", "barrels", "brl_percent","brl_pa")


Combined_MLB <- Combined_MLB %>%
  mutate(across(all_of(paste0(base_names_Off, ".Home.Off")), ~ . - get(gsub("Home", "Away", cur_column())), .names = "{gsub('Home.Off', 'Diff.Off', .col)}")) %>%
  mutate(across(all_of(paste0(base_names_pitch, ".Home.Pitch")), ~ . - get(gsub("Home.Pitch", "Away.Pitch", cur_column())), .names = "{gsub('Home.Pitch', 'Diff.Pitch', .col)}"))%>%
  mutate(across(all_of(paste0(base_names_Field, ".Home.Field")),~ . - get(gsub("Home", "Away", cur_column())),.names = "{gsub('Home.Field', 'Diff.Field', .col)}"))%>%
  mutate(across(all_of(paste0(base_names, ".Home")), ~ . - get(gsub("Home", "Away", cur_column())),.names = "{gsub('Home$', 'Diff', .col)}")) %>%
  mutate(across(all_of(paste0(base_names, ".Home.opp")),~ . - get(gsub("Home.opp", "Away.opp", cur_column())),.names = "{gsub('Home.opp$', 'Diff.opp', .col)}"))

Combined_MLB


```

#Exploratory Analysis







Will put in more visuals, I am working on this

```{r}
geom_point()
ggplot(Combined_MLB, aes(x = avg_hr_distance.Home, y = avg_hr_distance.Away, color = Winner)) +
geom_jitter()
```


#Model Creation / Assesment


This code was used to get all of the variables, we will probably want to narrow it down to specific variables for when we do the actual model. We need at least 5 but can do more

```{r}
diff_names <- names(Combined_MLB)[grepl("Diff", names(Combined_MLB))]

# Create a single string with all variable names concatenated with ' + '
variable_string <- paste(diff_names, collapse = " + ")

# Print the resulting string
print(variable_string)

#Create Differences Matrix
diff_matrix <- Combined_MLB |>
  dplyr::select(, c(diff_names)) |>
  mutate(Winner = Combined_MLB$Winner)
diff_matrix
```

I did some sample models to check for which variables are significant. We can decide which ones we want to work with from these.

```{r}
Batstats = glm(Winner ~ attempts.Diff + avg_hit_angle.Diff + anglesweetspotpercent.Diff + max_hit_speed.Diff + avg_hit_speed.Diff + ev50.Diff + fbld.Diff + gb.Diff + max_distance.Diff + avg_distance.Diff + avg_hr_distance.Diff + ev95plus.Diff + ev95percent.Diff + barrels.Diff + brl_percent.Diff + brl_pa.Diff + attempts.Diff.opp + avg_hit_angle.Diff.opp + anglesweetspotpercent.Diff.opp + max_hit_speed.Diff.opp + avg_hit_speed.Diff.opp + ev50.Diff.opp + fbld.Diff.opp + gb.Diff.opp + max_distance.Diff.opp + avg_distance.Diff.opp + avg_hr_distance.Diff.opp + ev95plus.Diff.opp + ev95percent.Diff.opp + barrels.Diff.opp + brl_percent.Diff.opp + brl_pa.Diff.opp, data = Combined_MLB, family = binomial)

Offense = glm(Winner ~ X.Bat.Diff.Off + BatAge.Diff.Off + R.G.Diff.Off + G.Diff.Off + PA.Diff.Off + AB.Diff.Off + R.Diff.Off + H.Diff.Off + X2B.Diff.Off + X3B.Diff.Off + HR.Diff.Off + RBI.Diff.Off + SB.Diff.Off + CS.Diff.Off + BB.Diff.Off + SO.Diff.Off + BA.Diff.Off + OBP.Diff.Off + SLG.Diff.Off + OPS.Diff.Off + OPS..Diff.Off + TB.Diff.Off + GDP.Diff.Off + HBP.Diff.Off + SH.Diff.Off + SF.Diff.Off + IBB.Diff.Off + LOB.Diff.Off , data = Combined_MLB, family = binomial)

Pitching = glm(Winner ~ X.P.Diff.Pitch + PAge.Diff.Pitch + RA.G.Diff.Pitch + W.Diff.Pitch + L.Diff.Pitch + W.L..Diff.Pitch + ERA.Diff.Pitch + G.Diff.Pitch + GS.Diff.Pitch + GF.Diff.Pitch + CG.Diff.Pitch + tSho.Diff.Pitch + cSho.Diff.Pitch + SV.Diff.Pitch + IP.Diff.Pitch + H.Diff.Pitch + R.Diff.Pitch + ER.Diff.Pitch + HR.Diff.Pitch + BB.Diff.Pitch + IBB.Diff.Pitch + SO.Diff.Pitch + HBP.Diff.Pitch + BK.Diff.Pitch + WP.Diff.Pitch + BF.Diff.Pitch + ERA..Diff.Pitch + FIP.Diff.Pitch + WHIP.Diff.Pitch + H9.Diff.Pitch + HR9.Diff.Pitch + BB9.Diff.Pitch + SO9.Diff.Pitch + SO.W.Diff.Pitch + LOB.Diff.Pitch , data = Combined_MLB, family = binomial)

Fielding = glm(Winner ~ X.Fld.Diff.Field + RA.G.Diff.Field + DefEff.Diff.Field + G.Diff.Field + GS.Diff.Field + CG.Diff.Field + Inn.Diff.Field + Ch.Diff.Field + PO.Diff.Field + A.Diff.Field + E.Diff.Field + DP.Diff.Field + Fld..Diff.Field + Rtot.Diff.Field + Rtot.yr.Diff.Field + Rdrs.Diff.Field + Rdrs.yr.Diff.Field + Rgood.Diff.Field, data = Combined_MLB, family = binomial)

summary(Batstats)
summary(Offense)
summary(Pitching)
summary(Fielding)
```







#Logistic Regression

I made a simple logistic regression with 5 variables. We do not need to use these ones and can definitely put more in once we decide on more accurate ones. The code should work just by replacing the variables. 

Right now it only does it on the training set and I am working on getting data for a test set. We can make the models without and then when we get the test data and just slightly modify code.


```{r}
Model = glm(Winner ~  PA.Diff.Off + avg_hr_distance.Diff + OBP.Diff.Off + fbld.Diff + DefEff.Diff.Field, data = Combined_MLB, family = binomial)
summary(Model)


thresholds <- seq(0.3, 0.7, by = 0.01)
accuracy_results <- numeric(length(thresholds))

for (i in seq_along(thresholds)) {
  threshold <- thresholds[i]
  phat <- predict(Model, newdata = Combined_MLB, type = "response")
  predicted_classes <- ifelse(phat > threshold, "Home", "Away")
  actual_classes <- factor(Combined_MLB$Winner)
  predicted_classes <- factor(predicted_classes, levels = levels(actual_classes))

  conf_matrix <- confusionMatrix(predicted_classes, actual_classes)
  accuracy_results[i] <- conf_matrix$overall['Accuracy']}

best_index <- which.max(accuracy_results)
best_threshold <- thresholds[best_index]
best_accuracy <- accuracy_results[best_index]

cat("Best Threshold:", best_threshold, "\n")
cat("Best Accuracy:", best_accuracy, "\n")



plot(thresholds, accuracy_results, type = "l", col = "blue", xlab = "Threshold", ylab = "Accuracy",
     main = "Threshold vs. Accuracy")
abline(v = best_threshold, col = "red", lty = 2)


phat <- predict(Model, newdata = Combined_MLB, type = "response")
threshold = 0.56

predicted_classes  <- ifelse(phat > threshold, "Home" , "Away")
actual_classes <- factor(Combined_MLB$Winner)
predicted_classes  <- factor(predicted_classes , levels = levels(actual_classes))

Matrix = confusionMatrix(predicted_classes ,actual_classes, positive = "Home")
Matrix
```



#Need 2 more types of models

We can use any of the class models to try and come up with 2 more models. Once we decide on variables we just need to input them into the model code and the rest should work.

# LDA and QDA Models for Classification
```{r}
#Models
lda.mod <- lda(Winner ~ PA.Diff.Off + avg_hr_distance.Diff + OBP.Diff.Off + fbld.Diff + DefEff.Diff.Field, data = Combined_MLB)
lda.mod

qda.mod <- qda(Winner ~ PA.Diff.Off + avg_hr_distance.Diff + OBP.Diff.Off + fbld.Diff + DefEff.Diff.Field, data = Combined_MLB)
qda.mod

#Predictions
# LDA
yhat_lda <- predict(lda.mod, newdata = Combined_MLB)
roc(predictor = yhat_lda$posterior[,1], response = Combined_MLB$Winner, plot = TRUE)
cm_lda <- confusionMatrix(yhat_lda$class, Combined_MLB$Winner, positive = "Home")

# QDA
yhat_qda <- predict(qda.mod, newdata = Combined_MLB)
roc(predictor = yhat_qda$posterior[,1], response = Combined_MLB$Winner, plot = TRUE)
cm_qda <- confusionMatrix(yhat_qda$class, Combined_MLB$Winner, positive = "Home")


cm_lda 
cm_qda
```
Area Under the Curve (AUC): For LDA, the AUC is 0.6136 and for QDA, it is 0.6172. Values closer to 1.0 indicate better model performance, with 0.5 representing a model with no discriminative power

Accuracy: Both models have similar accuracy around 57%, slightly above the no information rate (52.1%), which is the proportion of the most frequent class.

Sensitivity and Specificity: Sensitivity (true positive rate) and specificity (true negative rate) provide insights into how well the model predicts each class. Both models have moderate sensitivity and specificity, indicating a reasonable balance but room for improvement.

Kappa: This statistic measures agreement between the predicted and actual classifications, corrected for agreement that occurs by chance. A kappa value near 0 suggests minimal agreement beyond chance, whereas higher values indicate better performance. Both models show low kappa values, suggesting modest agreement.



```{r}
#Normalize the Data
predictors <- Combined_MLB |>
  dplyr::select(PA.Diff.Off, avg_hr_distance.Diff, OBP.Diff.Off, fbld.Diff, DefEff.Diff.Field)

preproc <- preProcess(predictors, method = "scale")

scaled_predictors <- predict(preproc, Combined_MLB)

norm_data <- as.data.frame(scaled_predictors)

norm_data$Winner <- Combined_MLB$Winner

# LDA and QDA with cross-validation
control <- trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)

lda_fit <- train(Winner ~ PA.Diff.Off + avg_hr_distance.Diff + OBP.Diff.Off + fbld.Diff + DefEff.Diff.Field, data = norm_data, method = "lda", trControl = control, metric = "ROC")

qda_fit <- train(Winner ~ PA.Diff.Off + avg_hr_distance.Diff + OBP.Diff.Off + fbld.Diff + DefEff.Diff.Field, data = norm_data, method = "qda", trControl = control, metric = "ROC")

lda_fit
qda_fit

```
Cross-validation (CV) was implemented to provide a more robust evaluation of the model by dividing the data into 10 folds (segments). Each fold acts as a validation set once, with the remaining used as the training set. This process reduces variance in the model evaluation by using different subsets of the data for training and testing.

ROC (Area Under the Curve):
- LDA: The ROC score is approximately 0.605.
- QDA: The ROC score is about 0.598.
- These scores are close to the AUC values observed in the non-normalized models, suggesting that normalization didn't significantly change the discriminative ability of the models. AUC values closer to 1 indicate better model performance; here, values near 0.6 suggest moderate discrimination.

# Random Forest for classification
```{r}
mod.forest.class <- randomForest(Winner ~ PA.Diff.Off + avg_hr_distance.Diff + OBP.Diff.Off + fbld.Diff + DefEff.Diff.Field, data = Combined_MLB, mtry = 5, importance = TRUE)
varImpPlot(mod.forest.class)

yhat_forest <- predict(mod.forest.class, newdata = Combined_MLB)
cm_forest <- confusionMatrix(yhat_forest, Combined_MLB$Winner, positive = "Home")

cm_forest
```
'mtry = 5' : This parameter specifies that each tree in the forest should consider five variables at each split. Given that you have exactly five predictors, each tree in the forest will consider all available variables for splitting at each node.

Accuracy : 71.89%
Kappa = 0.4344 : The Kappa statistic adjusts accuracy by accounting for the possibility of correct prediction by chance alone. A Kappa value of 0.4344 suggests moderate agreement between prediction and reality beyond chance.


```{r}
mtry_values <- c(2,3,4,5)
ntree_values <- c(100, 500, 1000)
maxnodes_values <- c(10, 20 ,30)

#Store best model metrics 
best_model <- NULL
best_accuracy <- 0
best_parameters <- list(mtry = NULL, ntree = NULL, maxnodes = NULL)
best_confusionMatrix <- NULL

for (mtry in mtry_values) {
  for (ntree in ntree_values) {
    for (maxnodes in maxnodes_values) {
      mod <- randomForest(Winner ~ PA.Diff.Off + avg_hr_distance.Diff + OBP.Diff.Off + fbld.Diff + DefEff.Diff.Field,
                          data = Combined_MLB,
                          mtry = mtry,
                          ntree = ntree, 
                          maxnodes = maxnodes,
                          importance = TRUE)
      
      predictions <- predict(mod, Combined_MLB)
      
      cm <- confusionMatrix(predictions, Combined_MLB$Winner, positive = "Home")
      accuracy <- cm$overall['Accuracy']
      
      if (accuracy > best_accuracy) {
        best_accuracy <- accuracy
        best_model <- mod
        best_parameters <- list(mtry = mtry, ntree = ntree, maxnodes = maxnodes)
        best_confusionMatrix <- cm
      }
    }
  }
}

print("Best Model Parameters: \n")
print(best_parameters)
print(paste("Best Accuracy: ", best_accuracy))
print(best_confusionMatrix)

```

# SVM for Classification
```{r}
mod.svm <- tune.svm(Winner ~ PA.Diff.Off + avg_hr_distance.Diff + OBP.Diff.Off + fbld.Diff + DefEff.Diff.Field,
                    data = Combined_MLB,
                    scale = TRUE,
                    cost = 2^seq(-5,5,2))
mod.svm$best.model

yhat.svm <- predict(mod.svm$best.model, newdata = Combined_MLB)
confusionMatrix(yhat.svm, Combined_MLB$Winner, positive = "Home")

```







#We will need to do some assessing on all of the models

Once we have decided on the variables we want and have a test set, then we can do some more of this to see how we did.


